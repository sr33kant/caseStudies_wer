{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train_data=pd.read_csv(\"../../../Users/sreek/Documents/wer_ai/case-studies/train.tsv\",delimiter='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Rows where numerical values ar NAN will be imputed while rows where\n",
    "categorical values are missing will be dropped.Dropping  rows where columns f_61,f_121,f_215, f_237 have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train_dropped=raw_train_data.dropna(subset=['f_61','f_121','f_215','f_237'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_train_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del raw_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(\"../../../Users/sreek/Documents/wer_ai/case-studies/test.tsv\",delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_data=raw_train_dropped.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_train_dropped.drop('target',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_num=raw_train_dropped.loc[:,raw_train_dropped.dtypes =='float64']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_cat=raw_train_dropped.loc[:,raw_train_dropped.dtypes == object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_features=X_train_cat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashing categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_hash(df, max_size=20):\n",
    "    for col in cat_features:\n",
    "        df[col+\"_hash\"] = df[col].apply(lambda x: hash(x)%max_size)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "X_train_cat_new=feature_hash(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "X_train_cat_new.drop(['f_61','f_121','f_215','f_237'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sreek\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\sreek\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_cat_scaled=StandardScaler().fit_transform(X_train_cat_new.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputer=Imputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_num_imputed=imputer.fit_transform(X_train_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_num_scaled=StandardScaler().fit_transform(X_train_num_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_train=np.hstack([X_train_num_scaled,X_train_cat_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pca=PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#m_train_reduced=pca.fit_transform(m_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75,  1.48,  2.21,  2.91,  3.6 ,  4.19,  4.77,  5.35,  5.92,\n",
       "        6.49,  7.05,  7.61,  8.17,  8.73,  9.28,  9.83, 10.38, 10.92,\n",
       "       11.46, 12.  , 12.54, 13.08, 13.61, 14.14, 14.67, 15.2 , 15.73,\n",
       "       16.26, 16.78, 17.3 , 17.82, 18.34, 18.85, 19.36, 19.87, 20.38,\n",
       "       20.89, 21.4 , 21.91, 22.42, 22.92, 23.42, 23.92, 24.42, 24.92,\n",
       "       25.42, 25.91, 26.4 , 26.89, 27.38, 27.87, 28.36, 28.84, 29.32,\n",
       "       29.8 , 30.28, 30.76, 31.23, 31.7 , 32.17, 32.64, 33.11, 33.58,\n",
       "       34.05, 34.51, 34.97, 35.43, 35.89, 36.35, 36.81, 37.27, 37.72,\n",
       "       38.17, 38.62, 39.07, 39.52, 39.97, 40.42, 40.87, 41.31, 41.75,\n",
       "       42.19, 42.63, 43.07, 43.51, 43.95, 44.38, 44.81, 45.24, 45.67,\n",
       "       46.1 , 46.53, 46.96, 47.39, 47.82, 48.24, 48.66, 49.08, 49.5 ,\n",
       "       49.92, 50.34, 50.75, 51.16, 51.57, 51.98, 52.39, 52.8 , 53.21,\n",
       "       53.62, 54.02, 54.42, 54.82, 55.22, 55.62, 56.02, 56.42, 56.82,\n",
       "       57.22, 57.62, 58.02, 58.41, 58.8 , 59.19, 59.58, 59.97, 60.36,\n",
       "       60.75, 61.13, 61.51, 61.89, 62.27, 62.65, 63.03, 63.41, 63.79,\n",
       "       64.17, 64.55, 64.92, 65.29, 65.66, 66.03, 66.4 , 66.77, 67.14,\n",
       "       67.51, 67.87, 68.23, 68.59, 68.95, 69.31, 69.67, 70.03, 70.39,\n",
       "       70.75, 71.1 , 71.45, 71.8 , 72.15, 72.5 , 72.85, 73.2 , 73.55,\n",
       "       73.9 , 74.24, 74.58, 74.92, 75.26, 75.6 , 75.94, 76.28, 76.62,\n",
       "       76.96, 77.29, 77.62, 77.95, 78.28, 78.61, 78.94, 79.27, 79.6 ,\n",
       "       79.93, 80.26, 80.58, 80.9 , 81.22, 81.54, 81.86, 82.18, 82.5 ,\n",
       "       82.82, 83.14, 83.45, 83.76, 84.07, 84.38, 84.69, 85.  , 85.31,\n",
       "       85.62, 85.93, 86.24, 86.54, 86.84, 87.14, 87.44, 87.74, 88.04,\n",
       "       88.34, 88.64, 88.94, 89.23, 89.52, 89.81, 90.1 , 90.39, 90.68,\n",
       "       90.97, 91.26, 91.55, 91.83, 92.11, 92.39, 92.67, 92.95, 93.23,\n",
       "       93.51, 93.78, 94.05, 94.32, 94.59, 94.86, 95.13])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 17, \n",
    "    'learning_rate': 0.10,\n",
    "    'verbose': 0 \n",
    "    }\n",
    "n_estimators = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l2: 24.5036\tvalid_1's l2: 27.9392\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\ttraining's l2: 22.3058\tvalid_1's l2: 25.7062\n",
      "[3]\ttraining's l2: 20.4068\tvalid_1's l2: 23.8014\n",
      "[4]\ttraining's l2: 18.8805\tvalid_1's l2: 22.3022\n",
      "[5]\ttraining's l2: 17.5646\tvalid_1's l2: 21.0486\n",
      "[6]\ttraining's l2: 16.4266\tvalid_1's l2: 20.0041\n",
      "[7]\ttraining's l2: 15.2652\tvalid_1's l2: 19.0002\n",
      "[8]\ttraining's l2: 14.4154\tvalid_1's l2: 18.1007\n",
      "[9]\ttraining's l2: 13.5965\tvalid_1's l2: 17.1685\n",
      "[10]\ttraining's l2: 12.9471\tvalid_1's l2: 16.6614\n",
      "[11]\ttraining's l2: 12.1685\tvalid_1's l2: 15.7766\n",
      "[12]\ttraining's l2: 11.5662\tvalid_1's l2: 15.2133\n",
      "[13]\ttraining's l2: 11.0297\tvalid_1's l2: 14.7169\n",
      "[14]\ttraining's l2: 10.43\tvalid_1's l2: 14.2144\n",
      "[15]\ttraining's l2: 10.0212\tvalid_1's l2: 13.8557\n",
      "[16]\ttraining's l2: 9.50336\tvalid_1's l2: 13.3873\n",
      "[17]\ttraining's l2: 9.09521\tvalid_1's l2: 13.0295\n",
      "[18]\ttraining's l2: 8.5833\tvalid_1's l2: 12.4161\n",
      "[19]\ttraining's l2: 8.25614\tvalid_1's l2: 12.1535\n",
      "[20]\ttraining's l2: 7.89398\tvalid_1's l2: 11.806\n",
      "[21]\ttraining's l2: 7.50858\tvalid_1's l2: 11.3894\n",
      "[22]\ttraining's l2: 7.27996\tvalid_1's l2: 11.2646\n",
      "[23]\ttraining's l2: 7.062\tvalid_1's l2: 11.1401\n",
      "[24]\ttraining's l2: 6.70828\tvalid_1's l2: 10.8473\n",
      "[25]\ttraining's l2: 6.41467\tvalid_1's l2: 10.5428\n",
      "[26]\ttraining's l2: 6.2316\tvalid_1's l2: 10.4404\n",
      "[27]\ttraining's l2: 6.02697\tvalid_1's l2: 10.3094\n",
      "[28]\ttraining's l2: 5.75285\tvalid_1's l2: 10.0538\n",
      "[29]\ttraining's l2: 5.52169\tvalid_1's l2: 9.83361\n",
      "[30]\ttraining's l2: 5.30408\tvalid_1's l2: 9.61817\n",
      "[31]\ttraining's l2: 5.0957\tvalid_1's l2: 9.43713\n",
      "[32]\ttraining's l2: 4.91742\tvalid_1's l2: 9.25391\n",
      "[33]\ttraining's l2: 4.79212\tvalid_1's l2: 9.22365\n",
      "[34]\ttraining's l2: 4.60383\tvalid_1's l2: 9.03324\n",
      "[35]\ttraining's l2: 4.49051\tvalid_1's l2: 8.95456\n",
      "[36]\ttraining's l2: 4.21607\tvalid_1's l2: 8.59481\n",
      "[37]\ttraining's l2: 4.10238\tvalid_1's l2: 8.56382\n",
      "[38]\ttraining's l2: 4.01173\tvalid_1's l2: 8.52521\n",
      "[39]\ttraining's l2: 3.88802\tvalid_1's l2: 8.41524\n",
      "[40]\ttraining's l2: 3.79736\tvalid_1's l2: 8.38629\n",
      "[41]\ttraining's l2: 3.70982\tvalid_1's l2: 8.35233\n",
      "[42]\ttraining's l2: 3.61968\tvalid_1's l2: 8.33277\n",
      "[43]\ttraining's l2: 3.51676\tvalid_1's l2: 8.24587\n",
      "[44]\ttraining's l2: 3.21039\tvalid_1's l2: 7.75145\n",
      "[45]\ttraining's l2: 3.13378\tvalid_1's l2: 7.7416\n",
      "[46]\ttraining's l2: 3.06365\tvalid_1's l2: 7.71983\n",
      "[47]\ttraining's l2: 3.00059\tvalid_1's l2: 7.68835\n",
      "[48]\ttraining's l2: 2.93355\tvalid_1's l2: 7.67418\n",
      "[49]\ttraining's l2: 2.85114\tvalid_1's l2: 7.56854\n",
      "[50]\ttraining's l2: 2.79066\tvalid_1's l2: 7.56419\n",
      "[51]\ttraining's l2: 2.71492\tvalid_1's l2: 7.50571\n",
      "[52]\ttraining's l2: 2.61769\tvalid_1's l2: 7.36387\n",
      "[53]\ttraining's l2: 2.56248\tvalid_1's l2: 7.35781\n",
      "[54]\ttraining's l2: 2.46883\tvalid_1's l2: 7.26084\n",
      "[55]\ttraining's l2: 2.40201\tvalid_1's l2: 7.18435\n",
      "[56]\ttraining's l2: 2.34475\tvalid_1's l2: 7.15211\n",
      "[57]\ttraining's l2: 2.29348\tvalid_1's l2: 7.12333\n",
      "[58]\ttraining's l2: 2.238\tvalid_1's l2: 7.09683\n",
      "[59]\ttraining's l2: 2.19121\tvalid_1's l2: 7.11621\n",
      "[60]\ttraining's l2: 2.13934\tvalid_1's l2: 7.11427\n",
      "[61]\ttraining's l2: 2.08576\tvalid_1's l2: 7.08475\n",
      "[62]\ttraining's l2: 2.04228\tvalid_1's l2: 7.06788\n",
      "[63]\ttraining's l2: 2.00169\tvalid_1's l2: 7.06856\n",
      "[64]\ttraining's l2: 1.96398\tvalid_1's l2: 7.05574\n",
      "[65]\ttraining's l2: 1.8715\tvalid_1's l2: 6.89856\n",
      "[66]\ttraining's l2: 1.83347\tvalid_1's l2: 6.90591\n",
      "[67]\ttraining's l2: 1.80044\tvalid_1's l2: 6.91702\n",
      "[68]\ttraining's l2: 1.76723\tvalid_1's l2: 6.92426\n",
      "[69]\ttraining's l2: 1.72121\tvalid_1's l2: 6.90927\n",
      "[70]\ttraining's l2: 1.68748\tvalid_1's l2: 6.89758\n",
      "[71]\ttraining's l2: 1.64969\tvalid_1's l2: 6.86648\n",
      "[72]\ttraining's l2: 1.59173\tvalid_1's l2: 6.79938\n",
      "[73]\ttraining's l2: 1.55993\tvalid_1's l2: 6.77911\n",
      "[74]\ttraining's l2: 1.52703\tvalid_1's l2: 6.77263\n",
      "[75]\ttraining's l2: 1.46441\tvalid_1's l2: 6.67178\n",
      "[76]\ttraining's l2: 1.43746\tvalid_1's l2: 6.69552\n",
      "[77]\ttraining's l2: 1.40945\tvalid_1's l2: 6.69217\n",
      "[78]\ttraining's l2: 1.38312\tvalid_1's l2: 6.68984\n",
      "[79]\ttraining's l2: 1.35065\tvalid_1's l2: 6.68656\n",
      "[80]\ttraining's l2: 1.31469\tvalid_1's l2: 6.63103\n",
      "[81]\ttraining's l2: 1.28319\tvalid_1's l2: 6.60924\n",
      "[82]\ttraining's l2: 1.25949\tvalid_1's l2: 6.61332\n",
      "[83]\ttraining's l2: 1.22692\tvalid_1's l2: 6.59271\n",
      "[84]\ttraining's l2: 1.19935\tvalid_1's l2: 6.5548\n",
      "[85]\ttraining's l2: 1.17648\tvalid_1's l2: 6.55986\n",
      "[86]\ttraining's l2: 1.15454\tvalid_1's l2: 6.56666\n",
      "[87]\ttraining's l2: 1.12627\tvalid_1's l2: 6.53991\n",
      "[88]\ttraining's l2: 1.10005\tvalid_1's l2: 6.53324\n",
      "[89]\ttraining's l2: 1.07489\tvalid_1's l2: 6.53229\n",
      "[90]\ttraining's l2: 1.05491\tvalid_1's l2: 6.52704\n",
      "[91]\ttraining's l2: 1.01633\tvalid_1's l2: 6.4598\n",
      "[92]\ttraining's l2: 0.995356\tvalid_1's l2: 6.44344\n",
      "[93]\ttraining's l2: 0.976871\tvalid_1's l2: 6.44103\n",
      "[94]\ttraining's l2: 0.95446\tvalid_1's l2: 6.41303\n",
      "[95]\ttraining's l2: 0.937782\tvalid_1's l2: 6.41297\n",
      "[96]\ttraining's l2: 0.920007\tvalid_1's l2: 6.44329\n",
      "[97]\ttraining's l2: 0.901671\tvalid_1's l2: 6.44341\n",
      "[98]\ttraining's l2: 0.882839\tvalid_1's l2: 6.44384\n",
      "[99]\ttraining's l2: 0.866293\tvalid_1's l2: 6.45309\n",
      "[100]\ttraining's l2: 0.849682\tvalid_1's l2: 6.44635\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's l2: 0.849682\tvalid_1's l2: 6.44635\n",
      "[1]\ttraining's l2: 25.0234\tvalid_1's l2: 25.3417\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\ttraining's l2: 22.8926\tvalid_1's l2: 23.4894\n",
      "[3]\ttraining's l2: 21.0015\tvalid_1's l2: 21.9172\n",
      "[4]\ttraining's l2: 19.4069\tvalid_1's l2: 20.5867\n",
      "[5]\ttraining's l2: 17.9286\tvalid_1's l2: 19.2355\n",
      "[6]\ttraining's l2: 16.6859\tvalid_1's l2: 18.254\n",
      "[7]\ttraining's l2: 15.6533\tvalid_1's l2: 17.4783\n",
      "[8]\ttraining's l2: 14.7771\tvalid_1's l2: 16.7793\n",
      "[9]\ttraining's l2: 13.8871\tvalid_1's l2: 16.0138\n",
      "[10]\ttraining's l2: 13.1308\tvalid_1's l2: 15.314\n",
      "[11]\ttraining's l2: 12.4731\tvalid_1's l2: 14.7995\n",
      "[12]\ttraining's l2: 11.8936\tvalid_1's l2: 14.4093\n",
      "[13]\ttraining's l2: 11.3228\tvalid_1's l2: 13.9152\n",
      "[14]\ttraining's l2: 10.8231\tvalid_1's l2: 13.4849\n",
      "[15]\ttraining's l2: 10.2612\tvalid_1's l2: 13.0444\n",
      "[16]\ttraining's l2: 9.64637\tvalid_1's l2: 12.4632\n",
      "[17]\ttraining's l2: 9.18555\tvalid_1's l2: 12.0561\n",
      "[18]\ttraining's l2: 8.84212\tvalid_1's l2: 11.785\n",
      "[19]\ttraining's l2: 8.5221\tvalid_1's l2: 11.5235\n",
      "[20]\ttraining's l2: 8.14416\tvalid_1's l2: 11.1994\n",
      "[21]\ttraining's l2: 7.7576\tvalid_1's l2: 10.805\n",
      "[22]\ttraining's l2: 7.36242\tvalid_1's l2: 10.3681\n",
      "[23]\ttraining's l2: 7.10811\tvalid_1's l2: 10.1628\n",
      "[24]\ttraining's l2: 6.89738\tvalid_1's l2: 10.0216\n",
      "[25]\ttraining's l2: 6.44135\tvalid_1's l2: 9.53407\n",
      "[26]\ttraining's l2: 6.19271\tvalid_1's l2: 9.23687\n",
      "[27]\ttraining's l2: 5.97879\tvalid_1's l2: 9.08001\n",
      "[28]\ttraining's l2: 5.74047\tvalid_1's l2: 8.88628\n",
      "[29]\ttraining's l2: 5.58073\tvalid_1's l2: 8.81611\n",
      "[30]\ttraining's l2: 5.29774\tvalid_1's l2: 8.51458\n",
      "[31]\ttraining's l2: 5.01637\tvalid_1's l2: 8.12756\n",
      "[32]\ttraining's l2: 4.87154\tvalid_1's l2: 7.9956\n",
      "[33]\ttraining's l2: 4.74899\tvalid_1's l2: 7.95965\n",
      "[34]\ttraining's l2: 4.61945\tvalid_1's l2: 7.85538\n",
      "[35]\ttraining's l2: 4.45984\tvalid_1's l2: 7.73135\n",
      "[36]\ttraining's l2: 4.28014\tvalid_1's l2: 7.54493\n",
      "[37]\ttraining's l2: 3.99452\tvalid_1's l2: 7.14975\n",
      "[38]\ttraining's l2: 3.84086\tvalid_1's l2: 7.00379\n",
      "[39]\ttraining's l2: 3.73559\tvalid_1's l2: 6.91911\n",
      "[40]\ttraining's l2: 3.62569\tvalid_1's l2: 6.81597\n",
      "[41]\ttraining's l2: 3.54463\tvalid_1's l2: 6.7672\n",
      "[42]\ttraining's l2: 3.40971\tvalid_1's l2: 6.60149\n",
      "[43]\ttraining's l2: 3.27126\tvalid_1's l2: 6.44425\n",
      "[44]\ttraining's l2: 3.09268\tvalid_1's l2: 6.17707\n",
      "[45]\ttraining's l2: 3.02335\tvalid_1's l2: 6.1421\n",
      "[46]\ttraining's l2: 2.95472\tvalid_1's l2: 6.12558\n",
      "[47]\ttraining's l2: 2.89465\tvalid_1's l2: 6.09786\n",
      "[48]\ttraining's l2: 2.80009\tvalid_1's l2: 6.00083\n",
      "[49]\ttraining's l2: 2.70106\tvalid_1's l2: 5.87599\n",
      "[50]\ttraining's l2: 2.64316\tvalid_1's l2: 5.84726\n",
      "[51]\ttraining's l2: 2.57902\tvalid_1's l2: 5.80164\n",
      "[52]\ttraining's l2: 2.52285\tvalid_1's l2: 5.78947\n",
      "[53]\ttraining's l2: 2.46937\tvalid_1's l2: 5.77804\n",
      "[54]\ttraining's l2: 2.41594\tvalid_1's l2: 5.76497\n",
      "[55]\ttraining's l2: 2.35003\tvalid_1's l2: 5.71955\n",
      "[56]\ttraining's l2: 2.30231\tvalid_1's l2: 5.70941\n",
      "[57]\ttraining's l2: 2.25724\tvalid_1's l2: 5.70151\n",
      "[58]\ttraining's l2: 2.21036\tvalid_1's l2: 5.68513\n",
      "[59]\ttraining's l2: 2.16664\tvalid_1's l2: 5.6866\n",
      "[60]\ttraining's l2: 2.01573\tvalid_1's l2: 5.43018\n",
      "[61]\ttraining's l2: 1.97425\tvalid_1's l2: 5.42951\n",
      "[62]\ttraining's l2: 1.87228\tvalid_1's l2: 5.25722\n",
      "[63]\ttraining's l2: 1.83272\tvalid_1's l2: 5.25639\n",
      "[64]\ttraining's l2: 1.74979\tvalid_1's l2: 5.12842\n",
      "[65]\ttraining's l2: 1.714\tvalid_1's l2: 5.12337\n",
      "[66]\ttraining's l2: 1.67819\tvalid_1's l2: 5.11377\n",
      "[67]\ttraining's l2: 1.64399\tvalid_1's l2: 5.11592\n",
      "[68]\ttraining's l2: 1.61125\tvalid_1's l2: 5.1088\n",
      "[69]\ttraining's l2: 1.54704\tvalid_1's l2: 5.00943\n",
      "[70]\ttraining's l2: 1.51537\tvalid_1's l2: 5.00296\n",
      "[71]\ttraining's l2: 1.47884\tvalid_1's l2: 4.97567\n",
      "[72]\ttraining's l2: 1.45037\tvalid_1's l2: 4.97108\n",
      "[73]\ttraining's l2: 1.42151\tvalid_1's l2: 4.96307\n",
      "[74]\ttraining's l2: 1.39233\tvalid_1's l2: 4.97384\n",
      "[75]\ttraining's l2: 1.36355\tvalid_1's l2: 4.97041\n",
      "[76]\ttraining's l2: 1.29194\tvalid_1's l2: 4.82247\n",
      "[77]\ttraining's l2: 1.2645\tvalid_1's l2: 4.82933\n",
      "[78]\ttraining's l2: 1.23993\tvalid_1's l2: 4.82137\n",
      "[79]\ttraining's l2: 1.21508\tvalid_1's l2: 4.81099\n",
      "[80]\ttraining's l2: 1.18928\tvalid_1's l2: 4.80299\n",
      "[81]\ttraining's l2: 1.16702\tvalid_1's l2: 4.80619\n",
      "[82]\ttraining's l2: 1.14398\tvalid_1's l2: 4.80688\n",
      "[83]\ttraining's l2: 1.12099\tvalid_1's l2: 4.81146\n",
      "[84]\ttraining's l2: 1.09835\tvalid_1's l2: 4.8136\n",
      "[85]\ttraining's l2: 1.07716\tvalid_1's l2: 4.82232\n",
      "[86]\ttraining's l2: 1.04815\tvalid_1's l2: 4.77889\n",
      "[87]\ttraining's l2: 1.02967\tvalid_1's l2: 4.78164\n",
      "[88]\ttraining's l2: 1.01085\tvalid_1's l2: 4.7827\n",
      "[89]\ttraining's l2: 0.989474\tvalid_1's l2: 4.77214\n",
      "[90]\ttraining's l2: 0.96809\tvalid_1's l2: 4.76587\n",
      "[91]\ttraining's l2: 0.946428\tvalid_1's l2: 4.76225\n",
      "[92]\ttraining's l2: 0.925563\tvalid_1's l2: 4.76623\n",
      "[93]\ttraining's l2: 0.907896\tvalid_1's l2: 4.76169\n",
      "[94]\ttraining's l2: 0.890289\tvalid_1's l2: 4.75899\n",
      "[95]\ttraining's l2: 0.873883\tvalid_1's l2: 4.75562\n",
      "[96]\ttraining's l2: 0.856362\tvalid_1's l2: 4.75582\n",
      "[97]\ttraining's l2: 0.817778\tvalid_1's l2: 4.65219\n",
      "[98]\ttraining's l2: 0.801108\tvalid_1's l2: 4.65666\n",
      "[99]\ttraining's l2: 0.786387\tvalid_1's l2: 4.66005\n",
      "[100]\ttraining's l2: 0.772664\tvalid_1's l2: 4.65375\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's l2: 0.772664\tvalid_1's l2: 4.65375\n",
      "[1]\ttraining's l2: 25.2128\tvalid_1's l2: 24.1927\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\ttraining's l2: 23.0153\tvalid_1's l2: 22.293\n",
      "[3]\ttraining's l2: 21.1801\tvalid_1's l2: 20.7704\n",
      "[4]\ttraining's l2: 19.6408\tvalid_1's l2: 19.4796\n",
      "[5]\ttraining's l2: 18.2393\tvalid_1's l2: 18.3824\n",
      "[6]\ttraining's l2: 16.9251\tvalid_1's l2: 17.2615\n",
      "[7]\ttraining's l2: 15.9258\tvalid_1's l2: 16.5106\n",
      "[8]\ttraining's l2: 14.9354\tvalid_1's l2: 15.6893\n",
      "[9]\ttraining's l2: 14.044\tvalid_1's l2: 15.0657\n",
      "[10]\ttraining's l2: 13.2249\tvalid_1's l2: 14.3854\n",
      "[11]\ttraining's l2: 12.5348\tvalid_1's l2: 13.9733\n",
      "[12]\ttraining's l2: 11.8954\tvalid_1's l2: 13.508\n",
      "[13]\ttraining's l2: 11.3699\tvalid_1's l2: 13.1062\n",
      "[14]\ttraining's l2: 10.9255\tvalid_1's l2: 12.8178\n",
      "[15]\ttraining's l2: 10.4587\tvalid_1's l2: 12.5217\n",
      "[16]\ttraining's l2: 9.97205\tvalid_1's l2: 12.0285\n",
      "[17]\ttraining's l2: 9.54466\tvalid_1's l2: 11.6314\n",
      "[18]\ttraining's l2: 9.11785\tvalid_1's l2: 11.2572\n",
      "[19]\ttraining's l2: 8.60632\tvalid_1's l2: 10.7866\n",
      "[20]\ttraining's l2: 8.23255\tvalid_1's l2: 10.4874\n",
      "[21]\ttraining's l2: 7.94688\tvalid_1's l2: 10.2658\n",
      "[22]\ttraining's l2: 7.60963\tvalid_1's l2: 10.0001\n",
      "[23]\ttraining's l2: 7.23924\tvalid_1's l2: 9.64916\n",
      "[24]\ttraining's l2: 7.02338\tvalid_1's l2: 9.52078\n",
      "[25]\ttraining's l2: 6.56169\tvalid_1's l2: 9.07494\n",
      "[26]\ttraining's l2: 6.13486\tvalid_1's l2: 8.58399\n",
      "[27]\ttraining's l2: 5.95517\tvalid_1's l2: 8.46011\n",
      "[28]\ttraining's l2: 5.74711\tvalid_1's l2: 8.27949\n",
      "[29]\ttraining's l2: 5.56923\tvalid_1's l2: 8.16373\n",
      "[30]\ttraining's l2: 5.35749\tvalid_1's l2: 8.03148\n",
      "[31]\ttraining's l2: 5.13292\tvalid_1's l2: 7.81984\n",
      "[32]\ttraining's l2: 4.90571\tvalid_1's l2: 7.62054\n",
      "[33]\ttraining's l2: 4.5864\tvalid_1's l2: 7.3107\n",
      "[34]\ttraining's l2: 4.46724\tvalid_1's l2: 7.26867\n",
      "[35]\ttraining's l2: 4.35234\tvalid_1's l2: 7.22356\n",
      "[36]\ttraining's l2: 4.21173\tvalid_1's l2: 7.16515\n",
      "[37]\ttraining's l2: 3.98703\tvalid_1's l2: 6.91484\n",
      "[38]\ttraining's l2: 3.87525\tvalid_1's l2: 6.82236\n",
      "[39]\ttraining's l2: 3.77612\tvalid_1's l2: 6.76697\n",
      "[40]\ttraining's l2: 3.6858\tvalid_1's l2: 6.76213\n",
      "[41]\ttraining's l2: 3.60004\tvalid_1's l2: 6.71387\n",
      "[42]\ttraining's l2: 3.52042\tvalid_1's l2: 6.70204\n",
      "[43]\ttraining's l2: 3.38062\tvalid_1's l2: 6.55221\n",
      "[44]\ttraining's l2: 3.291\tvalid_1's l2: 6.5212\n",
      "[45]\ttraining's l2: 3.21632\tvalid_1's l2: 6.48844\n",
      "[46]\ttraining's l2: 3.12283\tvalid_1's l2: 6.43063\n",
      "[47]\ttraining's l2: 3.04581\tvalid_1's l2: 6.40857\n",
      "[48]\ttraining's l2: 2.97275\tvalid_1's l2: 6.35425\n",
      "[49]\ttraining's l2: 2.85536\tvalid_1's l2: 6.22701\n",
      "[50]\ttraining's l2: 2.78137\tvalid_1's l2: 6.1592\n",
      "[51]\ttraining's l2: 2.7183\tvalid_1's l2: 6.15698\n",
      "[52]\ttraining's l2: 2.64489\tvalid_1's l2: 6.09845\n",
      "[53]\ttraining's l2: 2.48109\tvalid_1's l2: 5.88192\n",
      "[54]\ttraining's l2: 2.42673\tvalid_1's l2: 5.84965\n",
      "[55]\ttraining's l2: 2.35543\tvalid_1's l2: 5.79076\n",
      "[56]\ttraining's l2: 2.30536\tvalid_1's l2: 5.77186\n",
      "[57]\ttraining's l2: 2.2395\tvalid_1's l2: 5.7301\n",
      "[58]\ttraining's l2: 2.1923\tvalid_1's l2: 5.7084\n",
      "[59]\ttraining's l2: 2.13715\tvalid_1's l2: 5.66874\n",
      "[60]\ttraining's l2: 2.09317\tvalid_1's l2: 5.64097\n",
      "[61]\ttraining's l2: 2.04439\tvalid_1's l2: 5.638\n",
      "[62]\ttraining's l2: 1.99177\tvalid_1's l2: 5.5861\n",
      "[63]\ttraining's l2: 1.9247\tvalid_1's l2: 5.51241\n",
      "[64]\ttraining's l2: 1.88591\tvalid_1's l2: 5.50944\n",
      "[65]\ttraining's l2: 1.84794\tvalid_1's l2: 5.49125\n",
      "[66]\ttraining's l2: 1.81142\tvalid_1's l2: 5.48674\n",
      "[67]\ttraining's l2: 1.77729\tvalid_1's l2: 5.47855\n",
      "[68]\ttraining's l2: 1.74344\tvalid_1's l2: 5.4877\n",
      "[69]\ttraining's l2: 1.65287\tvalid_1's l2: 5.3437\n",
      "[70]\ttraining's l2: 1.61763\tvalid_1's l2: 5.34259\n",
      "[71]\ttraining's l2: 1.58685\tvalid_1's l2: 5.34063\n",
      "[72]\ttraining's l2: 1.55505\tvalid_1's l2: 5.35412\n",
      "[73]\ttraining's l2: 1.52539\tvalid_1's l2: 5.3483\n",
      "[74]\ttraining's l2: 1.46997\tvalid_1's l2: 5.27256\n",
      "[75]\ttraining's l2: 1.43966\tvalid_1's l2: 5.27014\n",
      "[76]\ttraining's l2: 1.41458\tvalid_1's l2: 5.27069\n",
      "[77]\ttraining's l2: 1.34943\tvalid_1's l2: 5.15983\n",
      "[78]\ttraining's l2: 1.32076\tvalid_1's l2: 5.15643\n",
      "[79]\ttraining's l2: 1.29239\tvalid_1's l2: 5.14348\n",
      "[80]\ttraining's l2: 1.26808\tvalid_1's l2: 5.13855\n",
      "[81]\ttraining's l2: 1.24381\tvalid_1's l2: 5.14512\n",
      "[82]\ttraining's l2: 1.22229\tvalid_1's l2: 5.14051\n",
      "[83]\ttraining's l2: 1.16341\tvalid_1's l2: 5.03367\n",
      "[84]\ttraining's l2: 1.13241\tvalid_1's l2: 5.00846\n",
      "[85]\ttraining's l2: 1.1124\tvalid_1's l2: 4.99946\n",
      "[86]\ttraining's l2: 1.08597\tvalid_1's l2: 4.99298\n",
      "[87]\ttraining's l2: 1.06602\tvalid_1's l2: 4.99531\n",
      "[88]\ttraining's l2: 1.04338\tvalid_1's l2: 5.00015\n",
      "[89]\ttraining's l2: 1.02163\tvalid_1's l2: 5.0012\n",
      "[90]\ttraining's l2: 1.00234\tvalid_1's l2: 4.9927\n",
      "[91]\ttraining's l2: 0.983334\tvalid_1's l2: 4.98441\n",
      "[92]\ttraining's l2: 0.962169\tvalid_1's l2: 4.96806\n",
      "[93]\ttraining's l2: 0.947122\tvalid_1's l2: 4.97014\n",
      "[94]\ttraining's l2: 0.928669\tvalid_1's l2: 4.95323\n",
      "[95]\ttraining's l2: 0.911537\tvalid_1's l2: 4.94931\n",
      "[96]\ttraining's l2: 0.894794\tvalid_1's l2: 4.95909\n",
      "[97]\ttraining's l2: 0.877378\tvalid_1's l2: 4.96394\n",
      "[98]\ttraining's l2: 0.860561\tvalid_1's l2: 4.95983\n",
      "[99]\ttraining's l2: 0.843958\tvalid_1's l2: 4.95768\n",
      "[100]\ttraining's l2: 0.829818\tvalid_1's l2: 4.96168\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's l2: 0.829818\tvalid_1's l2: 4.96168\n",
      "[1]\ttraining's l2: 24.8313\tvalid_1's l2: 25.8886\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\ttraining's l2: 22.741\tvalid_1's l2: 23.8956\n",
      "[3]\ttraining's l2: 20.9271\tvalid_1's l2: 22.2052\n",
      "[4]\ttraining's l2: 19.3747\tvalid_1's l2: 20.8667\n",
      "[5]\ttraining's l2: 17.9495\tvalid_1's l2: 19.6248\n",
      "[6]\ttraining's l2: 16.8117\tvalid_1's l2: 18.6078\n",
      "[7]\ttraining's l2: 15.6507\tvalid_1's l2: 17.5699\n",
      "[8]\ttraining's l2: 14.7309\tvalid_1's l2: 16.6931\n",
      "[9]\ttraining's l2: 13.9487\tvalid_1's l2: 16.066\n",
      "[10]\ttraining's l2: 13.2489\tvalid_1's l2: 15.4849\n",
      "[11]\ttraining's l2: 12.5972\tvalid_1's l2: 14.9625\n",
      "[12]\ttraining's l2: 12.065\tvalid_1's l2: 14.5491\n",
      "[13]\ttraining's l2: 11.5095\tvalid_1's l2: 14.0886\n",
      "[14]\ttraining's l2: 10.9467\tvalid_1's l2: 13.6272\n",
      "[15]\ttraining's l2: 10.2632\tvalid_1's l2: 12.9127\n",
      "[16]\ttraining's l2: 9.81492\tvalid_1's l2: 12.5137\n",
      "[17]\ttraining's l2: 9.2791\tvalid_1's l2: 12.0168\n",
      "[18]\ttraining's l2: 8.93185\tvalid_1's l2: 11.8256\n",
      "[19]\ttraining's l2: 8.44807\tvalid_1's l2: 11.3808\n",
      "[20]\ttraining's l2: 8.06767\tvalid_1's l2: 11.0981\n",
      "[21]\ttraining's l2: 7.74221\tvalid_1's l2: 10.7782\n",
      "[22]\ttraining's l2: 7.49033\tvalid_1's l2: 10.5777\n",
      "[23]\ttraining's l2: 7.2398\tvalid_1's l2: 10.4048\n",
      "[24]\ttraining's l2: 6.88236\tvalid_1's l2: 10.0937\n",
      "[25]\ttraining's l2: 6.57513\tvalid_1's l2: 9.82461\n",
      "[26]\ttraining's l2: 6.23354\tvalid_1's l2: 9.43825\n",
      "[27]\ttraining's l2: 5.88738\tvalid_1's l2: 9.07692\n",
      "[28]\ttraining's l2: 5.67731\tvalid_1's l2: 8.89904\n",
      "[29]\ttraining's l2: 5.46744\tvalid_1's l2: 8.73799\n",
      "[30]\ttraining's l2: 5.19073\tvalid_1's l2: 8.37617\n",
      "[31]\ttraining's l2: 5.02749\tvalid_1's l2: 8.24729\n",
      "[32]\ttraining's l2: 4.79186\tvalid_1's l2: 7.98726\n",
      "[33]\ttraining's l2: 4.66425\tvalid_1's l2: 7.92927\n",
      "[34]\ttraining's l2: 4.54341\tvalid_1's l2: 7.86854\n",
      "[35]\ttraining's l2: 4.42598\tvalid_1's l2: 7.82219\n",
      "[36]\ttraining's l2: 4.2475\tvalid_1's l2: 7.69577\n",
      "[37]\ttraining's l2: 4.14284\tvalid_1's l2: 7.6401\n",
      "[38]\ttraining's l2: 4.01031\tvalid_1's l2: 7.50481\n",
      "[39]\ttraining's l2: 3.90272\tvalid_1's l2: 7.4216\n",
      "[40]\ttraining's l2: 3.75995\tvalid_1's l2: 7.28011\n",
      "[41]\ttraining's l2: 3.67289\tvalid_1's l2: 7.24743\n",
      "[42]\ttraining's l2: 3.50497\tvalid_1's l2: 7.05617\n",
      "[43]\ttraining's l2: 3.23611\tvalid_1's l2: 6.65857\n",
      "[44]\ttraining's l2: 3.15626\tvalid_1's l2: 6.60714\n",
      "[45]\ttraining's l2: 3.07976\tvalid_1's l2: 6.54866\n",
      "[46]\ttraining's l2: 2.99929\tvalid_1's l2: 6.50483\n",
      "[47]\ttraining's l2: 2.9341\tvalid_1's l2: 6.49343\n",
      "[48]\ttraining's l2: 2.86381\tvalid_1's l2: 6.46123\n",
      "[49]\ttraining's l2: 2.78557\tvalid_1's l2: 6.45276\n",
      "[50]\ttraining's l2: 2.71968\tvalid_1's l2: 6.4369\n",
      "[51]\ttraining's l2: 2.65454\tvalid_1's l2: 6.41721\n",
      "[52]\ttraining's l2: 2.56385\tvalid_1's l2: 6.33837\n",
      "[53]\ttraining's l2: 2.36803\tvalid_1's l2: 6.00686\n",
      "[54]\ttraining's l2: 2.31738\tvalid_1's l2: 5.9796\n",
      "[55]\ttraining's l2: 2.261\tvalid_1's l2: 5.93778\n",
      "[56]\ttraining's l2: 2.21161\tvalid_1's l2: 5.94117\n",
      "[57]\ttraining's l2: 2.16023\tvalid_1's l2: 5.90272\n",
      "[58]\ttraining's l2: 2.11493\tvalid_1's l2: 5.88606\n",
      "[59]\ttraining's l2: 2.06189\tvalid_1's l2: 5.85678\n",
      "[60]\ttraining's l2: 1.94437\tvalid_1's l2: 5.67845\n",
      "[61]\ttraining's l2: 1.904\tvalid_1's l2: 5.67778\n",
      "[62]\ttraining's l2: 1.86361\tvalid_1's l2: 5.66853\n",
      "[63]\ttraining's l2: 1.82262\tvalid_1's l2: 5.67423\n",
      "[64]\ttraining's l2: 1.78654\tvalid_1's l2: 5.67697\n",
      "[65]\ttraining's l2: 1.74701\tvalid_1's l2: 5.67636\n",
      "[66]\ttraining's l2: 1.70572\tvalid_1's l2: 5.66009\n",
      "[67]\ttraining's l2: 1.6726\tvalid_1's l2: 5.66288\n",
      "[68]\ttraining's l2: 1.63755\tvalid_1's l2: 5.65359\n",
      "[69]\ttraining's l2: 1.58331\tvalid_1's l2: 5.59726\n",
      "[70]\ttraining's l2: 1.55444\tvalid_1's l2: 5.57831\n",
      "[71]\ttraining's l2: 1.52363\tvalid_1's l2: 5.58186\n",
      "[72]\ttraining's l2: 1.49363\tvalid_1's l2: 5.58071\n",
      "[73]\ttraining's l2: 1.46544\tvalid_1's l2: 5.582\n",
      "[74]\ttraining's l2: 1.43294\tvalid_1's l2: 5.57774\n",
      "[75]\ttraining's l2: 1.3956\tvalid_1's l2: 5.52157\n",
      "[76]\ttraining's l2: 1.36597\tvalid_1's l2: 5.50353\n",
      "[77]\ttraining's l2: 1.32602\tvalid_1's l2: 5.46075\n",
      "[78]\ttraining's l2: 1.29695\tvalid_1's l2: 5.46241\n",
      "[79]\ttraining's l2: 1.27151\tvalid_1's l2: 5.45097\n",
      "[80]\ttraining's l2: 1.24761\tvalid_1's l2: 5.4555\n",
      "[81]\ttraining's l2: 1.22401\tvalid_1's l2: 5.45821\n",
      "[82]\ttraining's l2: 1.19687\tvalid_1's l2: 5.42491\n",
      "[83]\ttraining's l2: 1.17415\tvalid_1's l2: 5.41821\n",
      "[84]\ttraining's l2: 1.15238\tvalid_1's l2: 5.41617\n",
      "[85]\ttraining's l2: 1.13305\tvalid_1's l2: 5.42842\n",
      "[86]\ttraining's l2: 1.11134\tvalid_1's l2: 5.43548\n",
      "[87]\ttraining's l2: 1.08834\tvalid_1's l2: 5.43201\n",
      "[88]\ttraining's l2: 1.06856\tvalid_1's l2: 5.43944\n",
      "[89]\ttraining's l2: 1.04247\tvalid_1's l2: 5.41851\n",
      "[90]\ttraining's l2: 1.02373\tvalid_1's l2: 5.41769\n",
      "[91]\ttraining's l2: 0.971914\tvalid_1's l2: 5.29332\n",
      "[92]\ttraining's l2: 0.953668\tvalid_1's l2: 5.28374\n",
      "[93]\ttraining's l2: 0.929618\tvalid_1's l2: 5.25495\n",
      "[94]\ttraining's l2: 0.912537\tvalid_1's l2: 5.24939\n",
      "[95]\ttraining's l2: 0.891583\tvalid_1's l2: 5.23004\n",
      "[96]\ttraining's l2: 0.875404\tvalid_1's l2: 5.22598\n",
      "[97]\ttraining's l2: 0.858783\tvalid_1's l2: 5.22242\n",
      "[98]\ttraining's l2: 0.841513\tvalid_1's l2: 5.23265\n",
      "[99]\ttraining's l2: 0.825258\tvalid_1's l2: 5.23554\n",
      "[100]\ttraining's l2: 0.812046\tvalid_1's l2: 5.23797\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's l2: 0.812046\tvalid_1's l2: 5.23797\n",
      "[1]\ttraining's l2: 25.0064\tvalid_1's l2: 25.4298\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\ttraining's l2: 22.8433\tvalid_1's l2: 23.573\n",
      "[3]\ttraining's l2: 21.048\tvalid_1's l2: 21.9379\n",
      "[4]\ttraining's l2: 19.5483\tvalid_1's l2: 20.6342\n",
      "[5]\ttraining's l2: 18.1995\tvalid_1's l2: 19.4651\n",
      "[6]\ttraining's l2: 17.108\tvalid_1's l2: 18.5674\n",
      "[7]\ttraining's l2: 16.1174\tvalid_1's l2: 17.6629\n",
      "[8]\ttraining's l2: 15.0987\tvalid_1's l2: 16.7705\n",
      "[9]\ttraining's l2: 14.126\tvalid_1's l2: 15.9645\n",
      "[10]\ttraining's l2: 13.2833\tvalid_1's l2: 15.1925\n",
      "[11]\ttraining's l2: 12.5268\tvalid_1's l2: 14.6182\n",
      "[12]\ttraining's l2: 11.8626\tvalid_1's l2: 14.1277\n",
      "[13]\ttraining's l2: 11.318\tvalid_1's l2: 13.7393\n",
      "[14]\ttraining's l2: 10.8534\tvalid_1's l2: 13.3901\n",
      "[15]\ttraining's l2: 10.3614\tvalid_1's l2: 13.0996\n",
      "[16]\ttraining's l2: 9.94036\tvalid_1's l2: 12.7788\n",
      "[17]\ttraining's l2: 9.44597\tvalid_1's l2: 12.4079\n",
      "[18]\ttraining's l2: 9.05758\tvalid_1's l2: 12.1201\n",
      "[19]\ttraining's l2: 8.64838\tvalid_1's l2: 11.7585\n",
      "[20]\ttraining's l2: 8.29895\tvalid_1's l2: 11.4732\n",
      "[21]\ttraining's l2: 7.97045\tvalid_1's l2: 11.2196\n",
      "[22]\ttraining's l2: 7.67792\tvalid_1's l2: 11.0145\n",
      "[23]\ttraining's l2: 7.11521\tvalid_1's l2: 10.4369\n",
      "[24]\ttraining's l2: 6.848\tvalid_1's l2: 10.1851\n",
      "[25]\ttraining's l2: 6.60572\tvalid_1's l2: 10.0339\n",
      "[26]\ttraining's l2: 6.30678\tvalid_1's l2: 9.7322\n",
      "[27]\ttraining's l2: 5.93562\tvalid_1's l2: 9.36452\n",
      "[28]\ttraining's l2: 5.72311\tvalid_1's l2: 9.23635\n",
      "[29]\ttraining's l2: 5.42742\tvalid_1's l2: 8.93309\n",
      "[30]\ttraining's l2: 5.05477\tvalid_1's l2: 8.55176\n",
      "[31]\ttraining's l2: 4.91777\tvalid_1's l2: 8.49724\n",
      "[32]\ttraining's l2: 4.70977\tvalid_1's l2: 8.27605\n",
      "[33]\ttraining's l2: 4.56318\tvalid_1's l2: 8.17153\n",
      "[34]\ttraining's l2: 4.44506\tvalid_1's l2: 8.13793\n",
      "[35]\ttraining's l2: 4.1973\tvalid_1's l2: 7.85899\n",
      "[36]\ttraining's l2: 4.06683\tvalid_1's l2: 7.75995\n",
      "[37]\ttraining's l2: 3.91285\tvalid_1's l2: 7.6077\n",
      "[38]\ttraining's l2: 3.77946\tvalid_1's l2: 7.52755\n",
      "[39]\ttraining's l2: 3.62125\tvalid_1's l2: 7.39305\n",
      "[40]\ttraining's l2: 3.53199\tvalid_1's l2: 7.32915\n",
      "[41]\ttraining's l2: 3.42376\tvalid_1's l2: 7.2705\n",
      "[42]\ttraining's l2: 3.33687\tvalid_1's l2: 7.23392\n",
      "[43]\ttraining's l2: 3.26372\tvalid_1's l2: 7.19683\n",
      "[44]\ttraining's l2: 3.15337\tvalid_1's l2: 7.1058\n",
      "[45]\ttraining's l2: 3.087\tvalid_1's l2: 7.07704\n",
      "[46]\ttraining's l2: 3.01215\tvalid_1's l2: 7.07231\n",
      "[47]\ttraining's l2: 2.94848\tvalid_1's l2: 7.05714\n",
      "[48]\ttraining's l2: 2.88008\tvalid_1's l2: 7.04494\n",
      "[49]\ttraining's l2: 2.82016\tvalid_1's l2: 7.03585\n",
      "[50]\ttraining's l2: 2.73687\tvalid_1's l2: 6.9814\n",
      "[51]\ttraining's l2: 2.66924\tvalid_1's l2: 6.93523\n",
      "[52]\ttraining's l2: 2.61144\tvalid_1's l2: 6.90952\n",
      "[53]\ttraining's l2: 2.55159\tvalid_1's l2: 6.9071\n",
      "[54]\ttraining's l2: 2.4827\tvalid_1's l2: 6.88561\n",
      "[55]\ttraining's l2: 2.41675\tvalid_1's l2: 6.85777\n",
      "[56]\ttraining's l2: 2.35739\tvalid_1's l2: 6.83946\n",
      "[57]\ttraining's l2: 2.31078\tvalid_1's l2: 6.85928\n",
      "[58]\ttraining's l2: 2.26817\tvalid_1's l2: 6.84041\n",
      "[59]\ttraining's l2: 2.09347\tvalid_1's l2: 6.54846\n",
      "[60]\ttraining's l2: 2.02719\tvalid_1's l2: 6.4781\n",
      "[61]\ttraining's l2: 1.98867\tvalid_1's l2: 6.45398\n",
      "[62]\ttraining's l2: 1.8778\tvalid_1's l2: 6.25176\n",
      "[63]\ttraining's l2: 1.84164\tvalid_1's l2: 6.2432\n",
      "[64]\ttraining's l2: 1.80173\tvalid_1's l2: 6.19335\n",
      "[65]\ttraining's l2: 1.76596\tvalid_1's l2: 6.19454\n",
      "[66]\ttraining's l2: 1.73284\tvalid_1's l2: 6.18851\n",
      "[67]\ttraining's l2: 1.69267\tvalid_1's l2: 6.17473\n",
      "[68]\ttraining's l2: 1.65836\tvalid_1's l2: 6.16338\n",
      "[69]\ttraining's l2: 1.6207\tvalid_1's l2: 6.14896\n",
      "[70]\ttraining's l2: 1.58918\tvalid_1's l2: 6.14662\n",
      "[71]\ttraining's l2: 1.55814\tvalid_1's l2: 6.1466\n",
      "[72]\ttraining's l2: 1.52938\tvalid_1's l2: 6.15098\n",
      "[73]\ttraining's l2: 1.49991\tvalid_1's l2: 6.14381\n",
      "[74]\ttraining's l2: 1.47086\tvalid_1's l2: 6.13014\n",
      "[75]\ttraining's l2: 1.42027\tvalid_1's l2: 6.06558\n",
      "[76]\ttraining's l2: 1.38426\tvalid_1's l2: 6.02483\n",
      "[77]\ttraining's l2: 1.30607\tvalid_1's l2: 5.85277\n",
      "[78]\ttraining's l2: 1.28222\tvalid_1's l2: 5.85629\n",
      "[79]\ttraining's l2: 1.22731\tvalid_1's l2: 5.75677\n",
      "[80]\ttraining's l2: 1.20505\tvalid_1's l2: 5.75777\n",
      "[81]\ttraining's l2: 1.18137\tvalid_1's l2: 5.76631\n",
      "[82]\ttraining's l2: 1.15862\tvalid_1's l2: 5.77986\n",
      "[83]\ttraining's l2: 1.13729\tvalid_1's l2: 5.77763\n",
      "[84]\ttraining's l2: 1.11195\tvalid_1's l2: 5.76151\n",
      "[85]\ttraining's l2: 1.09293\tvalid_1's l2: 5.76192\n",
      "[86]\ttraining's l2: 1.07312\tvalid_1's l2: 5.76313\n",
      "[87]\ttraining's l2: 1.0538\tvalid_1's l2: 5.75921\n",
      "[88]\ttraining's l2: 1.03157\tvalid_1's l2: 5.74725\n",
      "[89]\ttraining's l2: 1.01433\tvalid_1's l2: 5.74617\n",
      "[90]\ttraining's l2: 0.995677\tvalid_1's l2: 5.7499\n",
      "[91]\ttraining's l2: 0.977671\tvalid_1's l2: 5.75863\n",
      "[92]\ttraining's l2: 0.960839\tvalid_1's l2: 5.76052\n",
      "[93]\ttraining's l2: 0.942273\tvalid_1's l2: 5.76986\n",
      "[94]\ttraining's l2: 0.925299\tvalid_1's l2: 5.7633\n",
      "[95]\ttraining's l2: 0.909714\tvalid_1's l2: 5.75855\n",
      "[96]\ttraining's l2: 0.893178\tvalid_1's l2: 5.76935\n",
      "[97]\ttraining's l2: 0.87795\tvalid_1's l2: 5.76879\n",
      "[98]\ttraining's l2: 0.863187\tvalid_1's l2: 5.77952\n",
      "[99]\ttraining's l2: 0.847103\tvalid_1's l2: 5.78065\n",
      "[100]\ttraining's l2: 0.83132\tvalid_1's l2: 5.78312\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's l2: 0.83132\tvalid_1's l2: 5.78312\n",
      "RMSE of preds in  0.7910896398546521\n",
      "[  4   6   6  11   3  12   8   5   3   2   6   5   9   6   3   8   7  12\n",
      "  14   6   9   7   2   5   3   4   5   9   5   8   2   9   3   4   3  99\n",
      "  14   3  12   6   2  12   3   4   6   7   9  17   9   7   6   5   7   7\n",
      "  16   4   7   2   3   6   2   4   9   9   3   7   3  12   8  13   5  10\n",
      "  11   4  38  14  14   6   4   2   4   4   3  15   3  13   8   4   7   4\n",
      "   4   7   7 365   3   8  12   7   6   8   8   8   4   9   9   6   7   8\n",
      "  10   4   1   9   5  15   7   9   4   5   3   2   5   7   8   5   8   3\n",
      "   9   3  14   4   4   4   5   9   1   6   5   5   9   6   5   3   8   8\n",
      "  14  12   2   5   4   4   4   3   6  16   5   7   5   9   3  19   5   6\n",
      "   5   2   5   7   8  37   5   4   4  13   4 168   4   2   8   3  13  11\n",
      "   4   7   7   6  10   3   6  14   8   7   7   6   2  36   2   7   3   7\n",
      "   6   2   4   3   4 156   5   6   6   7   7   6   3   5  10   3   3 384\n",
      "  10  14   8   6   2  10   6   6   7   7   8   6   4   7   5   2   4   0\n",
      "   9   9   9   1   4   5   8   5   7   1   2   6   5   3   4   6 134   0\n",
      "   0  42]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "n_iters = 5\n",
    "for i in range(n_iters): \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(m_train, target_data, test_size=0.15, random_state=i)\n",
    "    d_train = lgbm.Dataset(x_train, label=y_train)\n",
    "    d_valid = lgbm.Dataset(x_valid, label=y_valid)\n",
    "    watchlist = [d_valid]\n",
    "\n",
    "    model = lgbm.train(params, d_train, n_estimators,valid_sets=[d_train,d_valid], early_stopping_rounds=20, verbose_eval=1)\n",
    "\n",
    "    preds = model.predict(x_valid)\n",
    "    \n",
    "  \n",
    "\n",
    "#print('rmse of prediction is:', mean_squared_error(y_valid, preds) ** 0.5)  \n",
    "\n",
    "\n",
    "print(\"RMSE of preds in \",explained_variance_score(y_valid,preds))\n",
    "print(model.feature_importance())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
